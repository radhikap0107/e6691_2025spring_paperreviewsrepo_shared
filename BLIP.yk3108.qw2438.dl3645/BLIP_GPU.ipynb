{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIBfayKqhLEI",
    "outputId": "80d5f154-6228-413b-d4a3-37187ee7460f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "Found existing installation: transformers 4.51.1\n",
      "Uninstalling transformers-4.51.1:\n",
      "  Successfully uninstalled transformers-4.51.1\n",
      "Found existing installation: tokenizers 0.21.1\n",
      "Uninstalling tokenizers-0.21.1:\n",
      "  Successfully uninstalled tokenizers-0.21.1\n",
      "Collecting transformers==4.28.0\n",
      "  Downloading transformers-4.28.0-py3-none-any.whl.metadata (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.0/110.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers==0.13.3\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting timm==0.4.12\n",
      "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting fairscale==0.4.4\n",
      "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pycocoevalcap\n",
      "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: fairscale\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292933 sha256=ed5998ecea752987059e59ebf77b93d3e1c7cf1d071cb34f7e53be7ad767e9c7\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/18/68/b84796fef74ea879bb05cc499235ccbd3ccf097f673420cd39\n",
      "Successfully built fairscale\n",
      "Installing collected packages: tokenizers, transformers, timm, pycocoevalcap, fairscale, datasets\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 1.0.15\n",
      "    Uninstalling timm-1.0.15:\n",
      "      Successfully uninstalled timm-1.0.15\n",
      "Successfully installed datasets-3.5.0 fairscale-0.4.4 pycocoevalcap-1.2 timm-0.4.12 tokenizers-0.13.3 transformers-4.28.0\n"
     ]
    }
   ],
   "source": [
    "print('Installing dependencies...')\n",
    "!pip uninstall -y transformers tokenizers\n",
    "!pip install transformers==4.28.0 tokenizers==0.13.3 timm==0.4.12 fairscale==0.4.4 pycocoevalcap datasets --no-dependencies --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "02ada2173c484a2bae99147c0534f651",
      "443064e3734348cb929cdf9eb3747040",
      "14455a0840bd43fc8b049451525adee5",
      "7a562db98cec427d87af2c21470f3b8b",
      "147c69ebd4184749b0d92a0fdc3bdf08",
      "3f27407f2a7449d1ab58e57ac65195e5",
      "86238117bc33439da79f18b5cd87b4e4",
      "81bdd4bf78c04566bfe0d89e88a451ec",
      "ca359fd3721143dab8c21a863f592b6c",
      "bd64d842b63d4415a79798160281a6b3",
      "528d01421de04958a21d3e71d03818f6",
      "414ce8010cda40488363e92e74304d06",
      "73acc0f1afd74d2895d2ef982b51a3ac",
      "7bd8b8be74fb4a0d85308a03d4fa728d",
      "565fab77967e421aabd5e77b7ed05a25",
      "143a3953876a42f4998370567e92b43c",
      "f8ed77dca0704de4ab6fbf8903e6658a",
      "7fbe703e38724081bcf8435fafd140b3",
      "b49d6d7ba0184d67bac0cac5db8217bf",
      "8ba9f7ea8f894b49b7d72824822badb7",
      "30d641c738ee4a1e89bf33d8baceaadd",
      "bce4aad1ee6d4e69b17bf89c5662c525",
      "8f314a52632143a7b41176edf1a956bb",
      "a795bbe316b7437092d3c3c46a7314cb",
      "2f112603a4b448de98c6962a5a3f7e3a",
      "f4988e70a671478e816e23ab0a836d54",
      "d694802e4b0043eaa4c28922fd1435d8",
      "fd004ffb5ab042169d24b6fefcee4299",
      "ec21026f902949af8f7b8329667b6050",
      "107d1e47118e4c758402bfb2f0bce5f3",
      "0ee0392922a94f0e9d71736d0ea4476f",
      "37dceafd247c4ff39309cbd3fc89f5aa",
      "531d264ae3184ff892536d3ac89d3448",
      "6bfc2a26c87240609c4ca30709b234f0",
      "0f99720589cc4a298dc0ba006c8bcc2d",
      "3b59b9355ec74d2696f202b0b19504a4",
      "28217a05de3a43658bf56580f015d0cb",
      "aa08c90f9d3d4d3a8e4ccdce52273456",
      "4cd3b6654df04cd885629706cf6964e6",
      "e1be61e69a15443e97d468de20984f82",
      "50ca00aaa38245dd95b52d6b60a92694",
      "4ee27ab9ad544f53a693a7993d6bfc37",
      "dae7dbabe6e44622b008b6e9ee11139e",
      "0d397672352b479d9ddf0aa41cc93624",
      "80ea18dd9476415881d7decbe5bd1152",
      "7333c4d1398a41f9950e58781728a759",
      "b3cff4288c264445b7c89fc121fdb6d4",
      "b04de11d2dad4a919a091080de4babcb",
      "46d20fd1778f44c3881a81908225ce51",
      "2f22d07311b14d2c99313934609b435f",
      "05cbb5e9c5f346c6b33674646ab8825a",
      "6f4db7534b224ed68e7a2bc4a4e0dbcd",
      "e716f147449148a0839e80e3b8fe9575",
      "aef843edfe834ba1a0c5cbd12b4c6c44",
      "524cdd3c4ee7406e9ec56129aaca62e4",
      "4f6cd0f8628f414bbef640d283b86b9b",
      "517ae4ea56684881827ef9e529783819",
      "d19186d3f9994e3b94d056e8707b63d5",
      "b96a4e139ebb46d7b0e59603d419f0ee",
      "f6c2ef114e8a4f4faf8aa8602dde52a3",
      "7b739197f2184990983ec6bb8a4671cf",
      "a5301654653c43ddbd8da570a589f108",
      "ad759ba3f8da4a7a98467c1904d6c05e",
      "ee1ccbd872db4756815254f8c5697c20",
      "1e47f4ce36f64ac5806b1f7aa7e96e30",
      "5c936708a3284b329e43be51e8023d28",
      "f8734581170847828667ef10758776a2",
      "291ecaedd8774ffeb134b223630708ba",
      "59f3d557ce7144c8a019b31579e7d669",
      "df86b40f569947e18fd98e482c5ca609",
      "b52af6efa9f94b3682df7bdfdfe1f0a8",
      "3396745f666d4a8693c6c0579fdb5776",
      "78c446340b044567ba5040fa6af1b805",
      "d72b971b48884601a71e82d56e2f7906",
      "abf304cee5854969ac68b8479525e8cb",
      "2e202dbd26ea4578800626c990c57127",
      "de9cecc7be7546ab9f512c1df158dde3"
     ]
    },
    "id": "Wdxd0fajgLJl",
    "outputId": "1d4a238c-7a08-4b1c-8d19-cae6d31637a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Colab, checking dependencies...\n",
      "Removing existing BLIP directory to avoid nesting...\n",
      "Cloning BLIP repo...\n",
      "Cloning into 'BLIP'...\n",
      "remote: Enumerating objects: 277, done.\u001b[K\n",
      "remote: Counting objects: 100% (183/183), done.\u001b[K\n",
      "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
      "remote: Total 277 (delta 145), reused 137 (delta 137), pack-reused 94 (from 1)\u001b[K\n",
      "Receiving objects: 100% (277/277), 7.04 MiB | 19.06 MiB/s, done.\n",
      "Resolving deltas: 100% (152/152), done.\n",
      "/content/BLIP\n",
      "Downloading COCO validation dataset...\n",
      "--2025-04-16 00:01:50--  http://images.cocodataset.org/zips/val2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.11.156, 16.15.217.168, 52.217.87.28, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.11.156|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 815585330 (778M) [application/zip]\n",
      "Saving to: ‘datasets/coco/val2017.zip’\n",
      "\n",
      "val2017.zip         100%[===================>] 777.80M  20.3MB/s    in 39s     \n",
      "\n",
      "2025-04-16 00:02:30 (19.8 MB/s) - ‘datasets/coco/val2017.zip’ saved [815585330/815585330]\n",
      "\n",
      "--2025-04-16 00:02:35--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.28.49, 3.5.16.53, 3.5.25.39, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.28.49|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252907541 (241M) [application/zip]\n",
      "Saving to: ‘datasets/coco/annotations_trainval2017.zip’\n",
      "\n",
      "annotations_trainva 100%[===================>] 241.19M  22.1MB/s    in 13s     \n",
      "\n",
      "2025-04-16 00:02:48 (19.0 MB/s) - ‘datasets/coco/annotations_trainval2017.zip’ saved [252907541/252907541]\n",
      "\n",
      "Setting up Flickr30K evaluation...\n",
      "Setting up VQA evaluation...\n",
      "Cloning into 'VQA'...\n",
      "remote: Enumerating objects: 296, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 296 (delta 0), reused 3 (delta 0), pack-reused 291 (from 1)\u001b[K\n",
      "Receiving objects: 100% (296/296), 1.75 MiB | 5.47 MiB/s, done.\n",
      "Resolving deltas: 100% (124/124), done.\n",
      "--2025-04-16 00:02:57--  https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.131.120, 54.231.163.176, 54.231.229.24, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.131.120|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10518930 (10M) [application/zip]\n",
      "Saving to: ‘datasets/vqa/v2_Annotations_Val_mscoco.zip’\n",
      "\n",
      "v2_Annotations_Val_ 100%[===================>]  10.03M  5.03MB/s    in 2.0s    \n",
      "\n",
      "2025-04-16 00:03:00 (5.03 MB/s) - ‘datasets/vqa/v2_Annotations_Val_mscoco.zip’ saved [10518930/10518930]\n",
      "\n",
      "--2025-04-16 00:03:00--  https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.131.120, 54.231.163.176, 54.231.229.24, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.131.120|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3494929 (3.3M) [application/zip]\n",
      "Saving to: ‘datasets/vqa/v2_Questions_Val_mscoco.zip’\n",
      "\n",
      "v2_Questions_Val_ms 100%[===================>]   3.33M  2.63MB/s    in 1.3s    \n",
      "\n",
      "2025-04-16 00:03:03 (2.63 MB/s) - ‘datasets/vqa/v2_Questions_Val_mscoco.zip’ saved [3494929/3494929]\n",
      "\n",
      "Using device: cuda\n",
      "==================================================\n",
      "BLIP Benchmark Evaluation - Running All Tests\n",
      "==================================================\n",
      "\n",
      "Evaluating BLIP Image Captioning with Hugging Face implementation...\n",
      "Using device: cuda\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Using sample size of 100 images (from total of 5000)\n",
      "Generating captions for 100 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ada2173c484a2bae99147c0534f651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics...\n",
      "{'testlen': 740, 'reflen': 903, 'guess': [740, 640, 540, 440], 'correct': [548, 282, 116, 48]}\n",
      "ratio: 0.8194905869315399\n",
      "Error calculating METEOR: could not convert string to float: b'5.0 11.0 2.0 5.0 3.0 3.0 2.0 2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.0 5.0 5.0'\n",
      "\n",
      "Example Generated Captions:\n",
      "Image: 000000301061.jpg\n",
      "Generated: a man is standing next to a truck with a baby elephant\n",
      "References: ['A person is moving green hay towards an elephant that is inside the back of a white truck.', 'A man pulls an elephant out of a truck.', 'An elephant in the back of a truck trailer.', 'An elephant is being fed while in a truck.', 'There is an elephant in side a truck trying to come out']\n",
      "\n",
      "Image: 000000261982.jpg\n",
      "Generated: a man riding a skateboard down a street\n",
      "References: ['A man flying through the air while riding a skateboard.', 'a guy having a spill on his skate board', 'A person on a skateboard does a flip in a parking lot.', 'A person is riding on a skateboard on the street.', 'a person on a skate board does a trick ']\n",
      "\n",
      "Image: 000000273760.jpg\n",
      "Generated: a man holding a tennis racket\n",
      "References: ['A man standing on top of a tennis court holding a racquet.', 'a tennis player on a court with a racket', 'a man on a field with a tennis racket in his hand', 'A man is dressed in blue playing tennis.', 'a man that has a tennis racket in his hand']\n",
      "\n",
      "Image: 000000248400.jpg\n",
      "Generated: a pizza box with writing\n",
      "References: ['A man opens pizza box and stares at the camera.', 'Man displaying open pizza box with message written on inside.', 'A man is pointing at a pizza box with a some words on the inner side.', 'A boy holding open and gesturing towards a pizza box kid with a message written on it.', 'a kid is showing a pizza with a joke written inside the box cover']\n",
      "\n",
      "Image: 000000382030.jpg\n",
      "Generated: a pair of flip flops\n",
      "References: ['A wig sitting next to a pair of shoes on top of a table.', 'Items are sitting about for Chinese dress up fashion.', 'A cluttered collection of various items including a wig, shoes, umbrella and a photo.', 'A pair of sandals, an umbrellas and other items are piled onto a table.', 'A table full of many diffrent house hold items']\n",
      "\n",
      "\n",
      "Comparison with Paper Results:\n",
      "Metric    Paper    Ours    Difference\n",
      "BLEU@4    39.7%    23.7%    -16.0%\n",
      "CIDEr     133.3    87.0    -46.3\n",
      "\n",
      "Detailed Metrics:\n",
      "Bleu_1: 59.41%\n",
      "Bleu_2: 45.83%\n",
      "Bleu_3: 33.08%\n",
      "Bleu_4: 23.72%\n",
      "METEOR: 0.00%\n",
      "ROUGE_L: 43.68%\n",
      "CIDEr: 86.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Meteor.__del__ at 0x790e634db420>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pycocoevalcap/meteor/meteor.py\", line 78, in __del__\n",
      "    self.lock.acquire()\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Visual Question Answering...\n",
      "Using sample size of 100 QA pairs (from total of 214354)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414ce8010cda40488363e92e74304d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f314a52632143a7b41176edf1a956bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfc2a26c87240609c4ca30709b234f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.35G/1.35G [00:46<00:00, 30.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\n",
      "Answering 100 questions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ea18dd9476415881d7decbe5bd1152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing datasets/coco/val2017/000000397303.jpg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 0\n",
      "Error processing datasets/coco/val2017/000000410221.jpg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 0\n",
      "Error processing datasets/coco/val2017/000000425226.jpg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 0\n",
      "Error processing datasets/coco/val2017/000000021604.jpg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 0\n",
      "Error processing datasets/coco/val2017/000000311303.jpg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 0\n",
      "Error processing datasets/coco/val2017/000000445658.jpg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 0\n",
      "Error processing datasets/coco/val2017/000000482970.jpg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 0\n",
      "Error processing datasets/coco/val2017/000000033104.jpg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 0\n",
      "Error processing datasets/coco/val2017/000000273551.jpg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 0\n",
      "Error processing datasets/coco/val2017/000000446522.jpg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 0\n",
      "Error processing datasets/coco/val2017/000000563281.jpg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 0\n",
      "\n",
      "Visual Question Answering - Verification Results\n",
      "Sample Size: 100 of 214354 (0.0%)\n",
      "Margin of Error: ±9.80% at 95% confidence level\n",
      "==================================================\n",
      "\n",
      "Example Question-Answer Pairs:\n",
      "\n",
      "Evaluating Image-Text Retrieval...\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Using sample size of 100 images (from total of 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.78G/1.78G [01:01<00:00, 30.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\n",
      "Preparing 100 images and captions for retrieval...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6cd0f8628f414bbef640d283b86b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating retrieval scores...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8734581170847828667ef10758776a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b2c893b93881>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[0;31m# Run all evaluations with the subset size you prefer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_all_evaluations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# You can adjust the subset_size as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;31m# Optionally, print or process the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b2c893b93881>\u001b[0m in \u001b[0;36mrun_all_evaluations\u001b[0;34m(subset_size)\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m     \u001b[0;31m# 3. Image-Text Retrieval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m     \u001b[0mretrieval_comparison\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieval_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_image_text_retrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[0;31m# 4. Zero-Shot Flickr30K Retrieval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b2c893b93881>\u001b[0m in \u001b[0;36mevaluate_image_text_retrieval\u001b[0;34m(sample_size)\u001b[0m\n\u001b[1;32m    757\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m                         \u001b[0;31m# Calculate ITC score (cosine similarity)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'itc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/BLIP/models/blip_itm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, caption, match_head)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmatch_head\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'itc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      \n\u001b[0m\u001b[1;32m     62\u001b[0m                                             return_dict = True, mode = 'text')                     \n\u001b[1;32m     63\u001b[0m             \u001b[0mimage_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/BLIP/models/med.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder, mode)\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    782\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/BLIP/models/med.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m                 )\n\u001b[1;32m    444\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    446\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/BLIP/models/med.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/BLIP/models/med.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     ):\n\u001b[0;32m--> 277\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/BLIP/models/med.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_cross_attention\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BLIP Benchmark Evaluation with Statistical Sampling - Fixed Version\n",
    "# This notebook evaluates BLIP models on standard benchmarks using statistically valid sample sizes\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add this at the beginning of your script to avoid redundant downloads\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('Running in Colab, checking dependencies...')\n",
    "\n",
    "    # Define paths\n",
    "    coco_img_path = 'datasets/coco/val2017'\n",
    "    coco_ann_path = 'datasets/coco/annotations'\n",
    "    vqa_ann_path = 'datasets/vqa'\n",
    "    flickr_path = 'datasets/flickr30k'\n",
    "\n",
    "    # Only install dependencies if needed\n",
    "    if os.path.exists('BLIP'):\n",
    "        print(\"Removing existing BLIP directory to avoid nesting...\")\n",
    "        !rm -rf BLIP\n",
    "\n",
    "    # Clone fresh\n",
    "    print(\"Cloning BLIP repo...\")\n",
    "    !git clone https://github.com/salesforce/BLIP.git\n",
    "    %cd BLIP\n",
    "\n",
    "    # Only download datasets if they don't exist\n",
    "    if not os.path.exists(f\"{coco_img_path}\") or len(os.listdir(f\"{coco_img_path}\")) == 0:\n",
    "        print('Downloading COCO validation dataset...')\n",
    "        !mkdir -p {coco_img_path}\n",
    "        !mkdir -p {coco_ann_path}\n",
    "\n",
    "        # Download COCO validation images (5K)\n",
    "        !wget -c http://images.cocodataset.org/zips/val2017.zip -P datasets/coco/\n",
    "        !unzip -q datasets/coco/val2017.zip -d datasets/coco/\n",
    "    else:\n",
    "        print(f'COCO validation images already exist in {coco_img_path}, skipping download.')\n",
    "\n",
    "    if not os.path.exists(f\"{coco_ann_path}/captions_val2017.json\"):\n",
    "        # Download COCO annotations\n",
    "        !wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P datasets/coco/\n",
    "        !unzip -q datasets/coco/annotations_trainval2017.zip -d datasets/coco/\n",
    "    else:\n",
    "        print(f'COCO annotations already exist in {coco_ann_path}, skipping download.')\n",
    "\n",
    "    # For Flickr30K evaluation\n",
    "    if not os.path.exists(flickr_path):\n",
    "        print('Setting up Flickr30K evaluation...')\n",
    "        !mkdir -p {flickr_path}\n",
    "        !mkdir -p {flickr_path}/annotations\n",
    "\n",
    "    # For VQA evaluation\n",
    "    if not os.path.exists('VQA'):\n",
    "        print('Setting up VQA evaluation...')\n",
    "        !git clone https://github.com/GT-Vision-Lab/VQA.git\n",
    "\n",
    "    if not os.path.exists(f\"{vqa_ann_path}/v2_mscoco_val2014_annotations.json\"):\n",
    "        # Download VQA annotations\n",
    "        !mkdir -p {vqa_ann_path}\n",
    "        !wget -c https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip -P {vqa_ann_path}/\n",
    "        !unzip -q {vqa_ann_path}/v2_Annotations_Val_mscoco.zip -d {vqa_ann_path}/\n",
    "        !wget -c https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip -P {vqa_ann_path}/\n",
    "        !unzip -q {vqa_ann_path}/v2_Questions_Val_mscoco.zip -d {vqa_ann_path}/\n",
    "    else:\n",
    "        print(f'VQA annotations already exist in {vqa_ann_path}, skipping download.')\n",
    "\n",
    "    # Install necessary packages for evaluation\n",
    "    !pip install -q pycocoevalcap\n",
    "    !pip install -q \"transformers>=4.28.0\" pillow evaluate\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths based on environment\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Colab environment\n",
    "    coco_img_path = 'datasets/coco/val2017'\n",
    "    coco_ann_path = 'datasets/coco/annotations'\n",
    "    vqa_ann_path = 'datasets/vqa'\n",
    "    flickr_path = 'datasets/flickr30k'\n",
    "else:\n",
    "    # Local environment\n",
    "    coco_img_path = '../datasets/coco/val2017'\n",
    "    coco_ann_path = '../datasets/coco/annotations'\n",
    "    vqa_ann_path = '../datasets/vqa'\n",
    "    flickr_path = '../datasets/flickr30k'\n",
    "\n",
    "# Helper function to calculate minimum sample size\n",
    "def calculate_sample_size(population_size, confidence_level=0.95, margin_of_error=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the minimum sample size required for statistical significance\n",
    "\n",
    "    Args:\n",
    "        population_size: Total number of items in the population\n",
    "        confidence_level: Desired confidence level (default: 0.95 for 95%)\n",
    "        margin_of_error: Acceptable margin of error (default: 0.05 for 5%)\n",
    "\n",
    "    Returns:\n",
    "        Minimum sample size required\n",
    "    \"\"\"\n",
    "    # Z-score for given confidence level\n",
    "    z_scores = {\n",
    "        0.90: 1.645,\n",
    "        0.95: 1.96,\n",
    "        0.99: 2.576\n",
    "    }\n",
    "    z = z_scores.get(confidence_level, 1.96)\n",
    "\n",
    "    # Calculate sample size using the formula\n",
    "    # n = (z²pq)/(E²)\n",
    "    # For maximum variance, p = q = 0.5\n",
    "    numerator = z**2 * 0.5 * (1-0.5)\n",
    "    denominator = margin_of_error**2\n",
    "    sample_size = numerator / denominator\n",
    "\n",
    "    # Finite population correction\n",
    "    if population_size is not None:\n",
    "        sample_size = (sample_size * population_size) / (sample_size + population_size - 1)\n",
    "\n",
    "    return math.ceil(sample_size)\n",
    "\n",
    "# Helper function to check if an image can be properly loaded and transformed\n",
    "def validate_image(img_path, transform):\n",
    "    \"\"\"Check if an image can be properly loaded and transformed\"\"\"\n",
    "    try:\n",
    "        # First check if we can open and decode the image\n",
    "        with Image.open(img_path) as img:\n",
    "            # Ensure the image has 3 channels (RGB)\n",
    "            if img.mode != 'RGB':\n",
    "                return False\n",
    "\n",
    "            # Try applying the transform to check for compatibility\n",
    "            tensor = transform(img)\n",
    "            if tensor.shape[0] != 3:  # Should be 3 channels (RGB)\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Image validation error for {img_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Helper function to compare our results with paper results\n",
    "def compare_results(paper_results, our_results, task_name, sample_size=None, population_size=None):\n",
    "    \"\"\"Create a comparison table between paper results and our verification\"\"\"\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': paper_results.keys(),\n",
    "        'Paper Result': paper_results.values(),\n",
    "        'Our Result': [our_results.get(k, 'N/A') for k in paper_results.keys()],\n",
    "        'Difference': [our_results.get(k, 0) - v if k in our_results else 'N/A'\n",
    "                      for k, v in paper_results.items()]\n",
    "    })\n",
    "\n",
    "    print(f\"\\n{task_name} - Verification Results\")\n",
    "    if sample_size and population_size:\n",
    "        margin_of_error = 1.96 * math.sqrt((0.5 * 0.5) / sample_size) * math.sqrt((population_size - sample_size) / (population_size - 1))\n",
    "        print(f\"Sample Size: {sample_size} of {population_size} ({(sample_size/population_size)*100:.1f}%)\")\n",
    "        print(f\"Margin of Error: ±{margin_of_error*100:.2f}% at 95% confidence level\")\n",
    "    print(\"=\"*50)\n",
    "    return comparison_df\n",
    "\n",
    "# =============================================\n",
    "# 1. Image Captioning Evaluation\n",
    "# =============================================\n",
    "\n",
    "def evaluate_blip_hf_captioning(sample_size=20):\n",
    "    \"\"\"\n",
    "    Evaluate BLIP image captioning using the Hugging Face transformers library\n",
    "\n",
    "    Args:\n",
    "        sample_size: Number of images to evaluate\n",
    "    \"\"\"\n",
    "    # Import after installation\n",
    "    from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "    import sys\n",
    "\n",
    "    # Try to import evaluation metrics, with fallback implementations\n",
    "    try:\n",
    "        from pycocoevalcap.bleu.bleu import Bleu\n",
    "        from pycocoevalcap.meteor.meteor import Meteor\n",
    "        from pycocoevalcap.rouge.rouge import Rouge\n",
    "        from pycocoevalcap.cider.cider import Cider\n",
    "    except ImportError:\n",
    "        print(\"Warning: pycocoevalcap not installed properly. Using fallback metrics.\")\n",
    "\n",
    "        # Implement simple BLEU score calculation as fallback\n",
    "        class Bleu:\n",
    "            def __init__(self, n=4):\n",
    "                self.n = n\n",
    "\n",
    "            def compute_score(self, refs, hyps):\n",
    "                # Calculate simple n-gram precision for n=1,2,3,4\n",
    "                scores = [0.0] * self.n\n",
    "                for i in range(self.n):\n",
    "                    n = i + 1\n",
    "                    total_matches = 0\n",
    "                    total_hyp_ngrams = 0\n",
    "\n",
    "                    for img_id in refs.keys():\n",
    "                        ref_sentences = refs[img_id]\n",
    "                        hyp_sentence = hyps[img_id][0]\n",
    "\n",
    "                        # Generate n-grams\n",
    "                        hyp_words = hyp_sentence.lower().split()\n",
    "                        hyp_ngrams = set()\n",
    "                        for j in range(len(hyp_words)-n+1):\n",
    "                            hyp_ngrams.add(tuple(hyp_words[j:j+n]))\n",
    "\n",
    "                        total_hyp_ngrams += len(hyp_ngrams)\n",
    "\n",
    "                        # Count matches with any reference\n",
    "                        matches = set()\n",
    "                        for ref in ref_sentences:\n",
    "                            ref_words = ref.lower().split()\n",
    "                            for j in range(len(ref_words)-n+1):\n",
    "                                ngram = tuple(ref_words[j:j+n])\n",
    "                                if ngram in hyp_ngrams:\n",
    "                                    matches.add(ngram)\n",
    "\n",
    "                        total_matches += len(matches)\n",
    "\n",
    "                    if total_hyp_ngrams > 0:\n",
    "                        scores[i] = total_matches / total_hyp_ngrams\n",
    "\n",
    "                return scores, scores\n",
    "\n",
    "        # Simple implementations for other metrics if needed\n",
    "        class Rouge:\n",
    "            def compute_score(self, refs, hyps):\n",
    "                score = 0.0\n",
    "                scores = []\n",
    "\n",
    "                for img_id in refs.keys():\n",
    "                    hyp = hyps[img_id][0].lower().split()\n",
    "                    best_score = 0\n",
    "\n",
    "                    for ref in refs[img_id]:\n",
    "                        ref_words = ref.lower().split()\n",
    "                        matches = set(hyp) & set(ref_words)\n",
    "                        recall = len(matches) / len(ref_words) if ref_words else 0\n",
    "                        prec = len(matches) / len(hyp) if hyp else 0\n",
    "                        if recall > 0 and prec > 0:\n",
    "                            f1 = 2 * recall * prec / (recall + prec)\n",
    "                            best_score = max(best_score, f1)\n",
    "\n",
    "                    scores.append(best_score)\n",
    "                    score += best_score\n",
    "\n",
    "                if len(refs) > 0:\n",
    "                    score /= len(refs)\n",
    "\n",
    "                return score, scores\n",
    "\n",
    "        class Cider:\n",
    "            def compute_score(self, refs, hyps):\n",
    "                score = 0.0\n",
    "                scores = []\n",
    "\n",
    "                # Very simplified CIDEr implementation\n",
    "                for img_id in refs.keys():\n",
    "                    hyp = hyps[img_id][0].lower().split()\n",
    "                    hyp_count = {}\n",
    "                    for w in hyp:\n",
    "                        hyp_count[w] = hyp_count.get(w, 0) + 1\n",
    "\n",
    "                    best_score = 0\n",
    "                    for ref in refs[img_id]:\n",
    "                        ref_words = ref.lower().split()\n",
    "                        ref_count = {}\n",
    "                        for w in ref_words:\n",
    "                            ref_count[w] = ref_count.get(w, 0) + 1\n",
    "\n",
    "                        # Calculate cosine similarity\n",
    "                        dot_product = sum(hyp_count.get(w, 0) * ref_count.get(w, 0) for w in set(hyp_count) | set(ref_count))\n",
    "                        hyp_mag = math.sqrt(sum(c**2 for c in hyp_count.values()))\n",
    "                        ref_mag = math.sqrt(sum(c**2 for c in ref_count.values()))\n",
    "\n",
    "                        if hyp_mag > 0 and ref_mag > 0:\n",
    "                            similarity = dot_product / (hyp_mag * ref_mag)\n",
    "                            best_score = max(best_score, similarity)\n",
    "\n",
    "                    scores.append(best_score)\n",
    "                    score += best_score\n",
    "\n",
    "                if len(refs) > 0:\n",
    "                    score /= len(refs)\n",
    "\n",
    "                return score, scores\n",
    "\n",
    "        # Skip METEOR which requires external Java dependencies\n",
    "        class Meteor:\n",
    "            def compute_score(self, refs, hyps):\n",
    "                print(\"METEOR metric requires Java. Skipping.\")\n",
    "                return 0.0, [0.0] * len(refs)\n",
    "\n",
    "    print(\"\\nEvaluating BLIP Image Captioning with Hugging Face implementation...\")\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load COCO validation set\n",
    "    from pycocotools.coco import COCO\n",
    "    coco_ann_file = f\"{coco_ann_path}/captions_val2017.json\"\n",
    "    coco = COCO(coco_ann_file)\n",
    "    img_ids = list(coco.imgs.keys())\n",
    "    population_size = len(img_ids)\n",
    "\n",
    "    print(f\"Using sample size of {sample_size} images (from total of {population_size})\")\n",
    "\n",
    "    # Use random subset for evaluation\n",
    "    random.seed(42)  # For reproducibility\n",
    "    subset_img_ids = random.sample(img_ids, sample_size)\n",
    "\n",
    "    # Load BLIP model and processor from Hugging Face\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "    # Generate captions for validation images\n",
    "    results = []\n",
    "\n",
    "    print(f\"Generating captions for {sample_size} images...\")\n",
    "    for img_id in tqdm(subset_img_ids):\n",
    "        # Load image\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = f\"{coco_img_path}/{img_info['file_name']}\"\n",
    "\n",
    "        try:\n",
    "            # Load image\n",
    "            raw_image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "            # Generate caption with BLIP\n",
    "            inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(**inputs, max_length=50)\n",
    "                caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                # Add result\n",
    "                results.append({\n",
    "                    'image_id': img_id,\n",
    "                    'caption': caption\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "\n",
    "    # Save results to file\n",
    "    result_file = 'caption_results_hf.json'\n",
    "    with open(result_file, 'w') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    # Create reference dictionary for evaluation\n",
    "    references = {}\n",
    "    for img_id in subset_img_ids:\n",
    "        references[img_id] = []\n",
    "        anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n",
    "        for ann in anns:\n",
    "            references[img_id].append(ann['caption'])\n",
    "\n",
    "    # Filter out any image IDs that have no generated captions\n",
    "    hypo = {res['image_id']: [res['caption']] for res in results if res['image_id'] in references}\n",
    "    ref = {img_id: references[img_id] for img_id in hypo.keys()}\n",
    "\n",
    "    # Check if we have any valid results\n",
    "    if len(hypo) == 0:\n",
    "        print(\"No valid captions generated. Skipping metric calculation.\")\n",
    "        scores = {\n",
    "            \"Bleu_1\": 0, \"Bleu_2\": 0, \"Bleu_3\": 0, \"Bleu_4\": 0,\n",
    "            \"METEOR\": 0, \"ROUGE_L\": 0, \"CIDEr\": 0\n",
    "        }\n",
    "    else:\n",
    "        # Ensure that both hypo and ref have the same set of keys\n",
    "        assert hypo.keys() == ref.keys(), \"Mismatch between generated and ground truth image IDs\"\n",
    "\n",
    "        # Evaluate using standard metrics\n",
    "        print(\"Calculating metrics...\")\n",
    "\n",
    "        # Setup metrics\n",
    "        scorers = [\n",
    "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "            (Meteor(), \"METEOR\"),\n",
    "            (Rouge(), \"ROUGE_L\"),\n",
    "            (Cider(), \"CIDEr\")\n",
    "        ]\n",
    "\n",
    "        # Calculate scores with error handling\n",
    "        scores = {}\n",
    "\n",
    "        # Format results for scoring\n",
    "        for scorer, method in scorers:\n",
    "            try:\n",
    "                score, scores_list = scorer.compute_score(ref, hypo)\n",
    "                if isinstance(method, list):\n",
    "                    for m, s in zip(method, score):\n",
    "                        scores[m] = s\n",
    "                else:\n",
    "                    scores[method] = score\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating {method}: {str(e)}\")\n",
    "                if isinstance(method, list):\n",
    "                    for m in method:\n",
    "                        scores[m] = 0  # Default to 0 on error\n",
    "                else:\n",
    "                    scores[method] = 0  # Default to 0 on error\n",
    "\n",
    "    # Show example captions\n",
    "    print(\"\\nExample Generated Captions:\")\n",
    "    for i in range(min(5, len(results))):\n",
    "        img_id = results[i]['image_id']\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "\n",
    "        print(f\"Image: {img_info['file_name']}\")\n",
    "        print(f\"Generated: {results[i]['caption']}\")\n",
    "        print(f\"References: {references[img_id]}\")\n",
    "        print()\n",
    "\n",
    "    # Report paper vs our results\n",
    "    paper_results = {\n",
    "        'Bleu_4': 39.7,\n",
    "        'CIDEr': 133.3\n",
    "    }\n",
    "\n",
    "    # Scale our results to match paper's format\n",
    "    our_results = {\n",
    "        'Bleu_4': scores.get('Bleu_4', 0) * 100,  # Convert to percentage\n",
    "        'CIDEr': scores.get('CIDEr', 0) * 100     # Scale to match paper\n",
    "    }\n",
    "\n",
    "    print(\"\\nComparison with Paper Results:\")\n",
    "    print(f\"Metric    Paper    Ours    Difference\")\n",
    "    print(f\"BLEU@4    {paper_results['Bleu_4']:.1f}%    {our_results['Bleu_4']:.1f}%    {our_results['Bleu_4'] - paper_results['Bleu_4']:.1f}%\")\n",
    "    print(f\"CIDEr     {paper_results['CIDEr']:.1f}    {our_results['CIDEr']:.1f}    {our_results['CIDEr'] - paper_results['CIDEr']:.1f}\")\n",
    "\n",
    "    print(\"\\nDetailed Metrics:\")\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"{metric}: {score*100 if 'Bleu' in metric else score*100:.2f}%\")\n",
    "\n",
    "    return results, scores\n",
    "\n",
    "# =============================================\n",
    "# 2. Visual Question Answering Evaluation\n",
    "# =============================================\n",
    "\n",
    "def evaluate_vqa(sample_size=None):\n",
    "    \"\"\"\n",
    "    Evaluate BLIP on Visual Question Answering\n",
    "\n",
    "    Args:\n",
    "        sample_size: Number of QA pairs to evaluate (if None, automatically calculate)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from models.blip_vqa import blip_vqa\n",
    "    except ImportError:\n",
    "        print(\"BLIP VQA module not found. This evaluation requires the BLIP codebase.\")\n",
    "        print(\"Reporting only paper results.\")\n",
    "\n",
    "        # Return placeholder values\n",
    "        paper_results = {\n",
    "            'VQA test-dev': 78.25,\n",
    "            'VQA test-std': 78.32\n",
    "        }\n",
    "\n",
    "        our_results = {\n",
    "            'VQA validation accuracy': 'N/A (requires BLIP codebase)'\n",
    "        }\n",
    "\n",
    "        comparison = compare_results(paper_results, our_results, \"Visual Question Answering\", 0, 0)\n",
    "        return comparison, 0, []\n",
    "\n",
    "    import json\n",
    "\n",
    "    print(\"\\nEvaluating Visual Question Answering...\")\n",
    "\n",
    "    # Load VQA validation set\n",
    "    vqa_ann_file = f\"{vqa_ann_path}/v2_mscoco_val2014_annotations.json\"\n",
    "    vqa_ques_file = f\"{vqa_ann_path}/v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    "\n",
    "    try:\n",
    "        with open(vqa_ann_file, 'r') as f:\n",
    "            vqa_anns = json.load(f)['annotations']\n",
    "\n",
    "        with open(vqa_ques_file, 'r') as f:\n",
    "            vqa_questions = json.load(f)['questions']\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error loading VQA data: {str(e)}\")\n",
    "        print(\"This evaluation requires the VQA dataset.\")\n",
    "\n",
    "        # Return placeholder values\n",
    "        paper_results = {\n",
    "            'VQA test-dev': 78.25,\n",
    "            'VQA test-std': 78.32\n",
    "        }\n",
    "\n",
    "        our_results = {\n",
    "            'VQA validation accuracy': 'N/A (requires VQA dataset)'\n",
    "        }\n",
    "\n",
    "        comparison = compare_results(paper_results, our_results, \"Visual Question Answering\", 0, 0)\n",
    "        return comparison, 0, []\n",
    "\n",
    "    # Create question lookup\n",
    "    question_map = {q['question_id']: q for q in vqa_questions}\n",
    "\n",
    "    # Calculate appropriate sample size\n",
    "    population_size = len(vqa_anns)\n",
    "    if sample_size is None:\n",
    "        sample_size = calculate_sample_size(population_size)\n",
    "    print(f\"Using sample size of {sample_size} QA pairs (from total of {population_size})\")\n",
    "\n",
    "    # Use random subset for evaluation\n",
    "    random.seed(42)  # For reproducibility\n",
    "    subset_anns = random.sample(vqa_anns, sample_size)\n",
    "\n",
    "    # Load BLIP model\n",
    "    image_size = 480\n",
    "    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
    "\n",
    "    try:\n",
    "        model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading BLIP VQA model: {str(e)}\")\n",
    "        print(\"This evaluation requires the BLIP codebase and model weights.\")\n",
    "\n",
    "        # Return placeholder values\n",
    "        paper_results = {\n",
    "            'VQA test-dev': 78.25,\n",
    "            'VQA test-std': 78.32\n",
    "        }\n",
    "\n",
    "        our_results = {\n",
    "            'VQA validation accuracy': 'N/A (requires BLIP model)'\n",
    "        }\n",
    "\n",
    "        comparison = compare_results(paper_results, our_results, \"Visual Question Answering\", 0, 0)\n",
    "        return comparison, 0, []\n",
    "\n",
    "    # Define image transform\n",
    "    from torchvision import transforms\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "\n",
    "    # Generate answers for questions\n",
    "    results = []\n",
    "\n",
    "    print(f\"Answering {sample_size} questions...\")\n",
    "    for ann in tqdm(subset_anns):\n",
    "        question_id = ann['question_id']\n",
    "        img_id = ann['image_id']\n",
    "\n",
    "        # Get question\n",
    "        if question_id not in question_map:\n",
    "            print(f\"Question ID {question_id} not found in question map\")\n",
    "            continue\n",
    "\n",
    "        question = question_map[question_id]['question']\n",
    "\n",
    "        # Load image\n",
    "        # Note: VQA uses COCO 2014, adjust file path pattern based on your setup\n",
    "        img_path = f\"{coco_img_path}/{img_id:012d}.jpg\"\n",
    "        if not os.path.exists(img_path):\n",
    "            # Try alternate path format (depends on your dataset setup)\n",
    "            img_path = f\"{coco_img_path}/COCO_val2014_{img_id:012d}.jpg\"\n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            raw_image = Image.open(img_path).convert('RGB')\n",
    "            image = transform(raw_image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate answer\n",
    "            with torch.no_grad():\n",
    "                answer = model(image, question, train=False, inference='generate')\n",
    "\n",
    "                # Add result\n",
    "                results.append({\n",
    "                    'question_id': question_id,\n",
    "                    'answer': answer[0]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "\n",
    "    # Calculate VQA accuracy using the standard VQA evaluation formula\n",
    "    # VQA score = min(# humans that said answer / 3, 1)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for result in results:\n",
    "        question_id = result['question_id']\n",
    "        pred_answer = result['answer'].lower()\n",
    "\n",
    "        # Find annotation\n",
    "        for ann in subset_anns:\n",
    "            if ann['question_id'] == question_id:\n",
    "                # Count matches with ground truth answers\n",
    "                match_count = sum(1 for a in ann['answers'] if a['answer'].lower() == pred_answer)\n",
    "                # Use VQA accuracy formula\n",
    "                accuracy = min(match_count / 3, 1)\n",
    "                correct += accuracy\n",
    "                total += 1\n",
    "                break\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    # Reported results from BLIP paper for VQA (ViT-B with CapFilt-L on 129M images)\n",
    "    paper_results = {\n",
    "        'VQA test-dev': 78.25,\n",
    "        'VQA test-std': 78.32\n",
    "    }\n",
    "\n",
    "    # Our verification results\n",
    "    our_results = {\n",
    "        'VQA validation accuracy': accuracy * 100  # Convert to percentage\n",
    "    }\n",
    "\n",
    "    # Compare results\n",
    "    comparison = compare_results(paper_results, our_results, \"Visual Question Answering\",\n",
    "                                sample_size, population_size)\n",
    "\n",
    "    # Show example Q&A\n",
    "    print(\"\\nExample Question-Answer Pairs:\")\n",
    "    for i in range(min(5, len(results))):\n",
    "        result = results[i]\n",
    "        question_id = result['question_id']\n",
    "\n",
    "        # Find corresponding question and annotation\n",
    "        question = question_map[question_id]['question'] if question_id in question_map else \"Question not found\"\n",
    "\n",
    "        # Find ground truth answers\n",
    "        ground_truth = []\n",
    "        for ann in subset_anns:\n",
    "            if ann['question_id'] == question_id:\n",
    "                ground_truth = [a['answer'] for a in ann['answers']]\n",
    "                break\n",
    "\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Predicted: {result['answer']}\")\n",
    "        print(f\"Ground Truth: {ground_truth}\")\n",
    "        print()\n",
    "\n",
    "    return comparison, accuracy, results\n",
    "\n",
    "# =============================================\n",
    "# 3. Image-Text Retrieval Evaluation\n",
    "# =============================================\n",
    "\n",
    "def evaluate_image_text_retrieval(sample_size=None):\n",
    "    \"\"\"\n",
    "    Evaluate BLIP on Image-Text Retrieval\n",
    "\n",
    "    Args:\n",
    "        sample_size: Number of images to evaluate (if None, automatically calculate)\n",
    "    \"\"\"\n",
    "    from models.blip_itm import blip_itm\n",
    "    from pycocotools.coco import COCO\n",
    "\n",
    "    print(\"\\nEvaluating Image-Text Retrieval...\")\n",
    "\n",
    "    # Load COCO validation set\n",
    "    coco_ann_file = f\"{coco_ann_path}/captions_val2017.json\"\n",
    "    coco = COCO(coco_ann_file)\n",
    "    img_ids = list(coco.imgs.keys())\n",
    "    population_size = len(img_ids)\n",
    "\n",
    "    # Calculate appropriate sample size if not provided\n",
    "    if sample_size is None:\n",
    "        # For retrieval, we use a smaller sample since each image requires comparing\n",
    "        # with all captions, which is computationally intensive\n",
    "        sample_size = min(calculate_sample_size(population_size), 500)\n",
    "    print(f\"Using sample size of {sample_size} images (from total of {population_size})\")\n",
    "\n",
    "    # Use random subset for evaluation\n",
    "    random.seed(42)  # For reproducibility\n",
    "    subset_img_ids = random.sample(img_ids, sample_size)\n",
    "\n",
    "    # Load BLIP model\n",
    "    image_size = 384\n",
    "    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n",
    "\n",
    "    model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define image transform\n",
    "    from torchvision import transforms\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "\n",
    "    # Prepare data for retrieval evaluation\n",
    "    images = []\n",
    "    captions = []\n",
    "    image_ids = []\n",
    "\n",
    "    print(f\"Preparing {sample_size} images and captions for retrieval...\")\n",
    "    for img_id in tqdm(subset_img_ids):\n",
    "        # Load image\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = f\"{coco_img_path}/{img_info['file_name']}\"\n",
    "\n",
    "        try:\n",
    "            raw_image = Image.open(img_path).convert('RGB')\n",
    "            image = transform(raw_image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Get first caption for simplicity and to avoid memory issues\n",
    "            anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n",
    "            if len(anns) > 0:\n",
    "                images.append(image)\n",
    "                captions.append(anns[0]['caption'])\n",
    "                image_ids.append(img_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "\n",
    "    # Calculate similarity scores for all image-text pairs\n",
    "    # Note: For large sets, this is very memory intensive\n",
    "    # We use a batched approach to avoid OOM issues\n",
    "\n",
    "    batch_size = 100\n",
    "    num_images = len(images)\n",
    "    scores = torch.zeros(num_images, num_images)\n",
    "\n",
    "    print(\"Calculating retrieval scores...\")\n",
    "    for i in tqdm(range(0, num_images, batch_size)):\n",
    "        i_end = min(i + batch_size, num_images)\n",
    "        for j in range(0, num_images, batch_size):\n",
    "            j_end = min(j + batch_size, num_images)\n",
    "\n",
    "            for i_idx in range(i, i_end):\n",
    "                for j_idx in range(j, j_end):\n",
    "                    with torch.no_grad():\n",
    "                        # Calculate ITC score (cosine similarity)\n",
    "                        score = model(images[i_idx], captions[j_idx], match_head='itc').item()\n",
    "                        scores[i_idx, j_idx] = score\n",
    "\n",
    "    # Calculate retrieval metrics\n",
    "    # IR@K: Image Retrieval at K\n",
    "    # TR@K: Text Retrieval at K\n",
    "    ks = [1, 5, 10]\n",
    "\n",
    "    # Text Retrieval: For each image, find the top K matching texts\n",
    "    tr_correct = {k: 0 for k in ks}\n",
    "    for i in range(num_images):\n",
    "        # Get scores for this image\n",
    "        image_scores = scores[i]\n",
    "\n",
    "        # Sort scores\n",
    "        sorted_indices = torch.argsort(image_scores, descending=True)\n",
    "\n",
    "        # Check if ground truth caption is in top K\n",
    "        for k in ks:\n",
    "            if i in sorted_indices[:k]:\n",
    "                tr_correct[k] += 1\n",
    "\n",
    "    # Image Retrieval: For each text, find the top K matching images\n",
    "    ir_correct = {k: 0 for k in ks}\n",
    "    for j in range(num_images):\n",
    "        # Get scores for this caption\n",
    "        caption_scores = scores[:, j]\n",
    "\n",
    "        # Sort scores\n",
    "        sorted_indices = torch.argsort(caption_scores, descending=True)\n",
    "\n",
    "        # Check if ground truth image is in top K\n",
    "        for k in ks:\n",
    "            if j in sorted_indices[:k]:\n",
    "                ir_correct[k] += 1\n",
    "\n",
    "    # Calculate recall@K\n",
    "    tr_recall = {k: tr_correct[k] / num_images * 100 for k in ks}\n",
    "    ir_recall = {k: ir_correct[k] / num_images * 100 for k in ks}\n",
    "\n",
    "    # Reported results from BLIP paper for Image-Text Retrieval (BLIP with ViT-B on 129M images)\n",
    "    paper_results = {\n",
    "        'TR@1': 81.9,\n",
    "        'TR@5': 95.4,\n",
    "        'TR@10': 97.8,\n",
    "        'IR@1': 64.3,\n",
    "        'IR@5': 85.7,\n",
    "        'IR@10': 91.5\n",
    "    }\n",
    "\n",
    "    # Our verification results\n",
    "    our_results = {\n",
    "        'TR@1': tr_recall[1],\n",
    "        'TR@5': tr_recall[5],\n",
    "        'TR@10': tr_recall[10],\n",
    "        'IR@1': ir_recall[1],\n",
    "        'IR@5': ir_recall[5],\n",
    "        'IR@10': ir_recall[10]\n",
    "    }\n",
    "\n",
    "    # Compare results\n",
    "    comparison = compare_results(paper_results, our_results, \"Image-Text Retrieval\",\n",
    "                                len(images), population_size)\n",
    "\n",
    "    # Show example retrieval\n",
    "    print(\"\\nExample Image-Text Retrieval:\")\n",
    "    for i in range(min(5, len(images))):\n",
    "        # Get top 3 captions for this image\n",
    "        image_scores = scores[i]\n",
    "        sorted_indices = torch.argsort(image_scores, descending=True)\n",
    "\n",
    "        print(f\"Image ID: {image_ids[i]}\")\n",
    "        print(f\"Ground Truth Caption: {captions[i]}\")\n",
    "        print(\"Top 3 Retrieved Captions:\")\n",
    "        for j in range(3):\n",
    "            idx = sorted_indices[j].item()\n",
    "            print(f\"  {j+1}. {captions[idx]} (score: {image_scores[idx]:.4f})\")\n",
    "        print()\n",
    "\n",
    "    return comparison, (tr_recall, ir_recall)\n",
    "\n",
    "# =============================================\n",
    "# 4. Zero-Shot Flickr30K Evaluation\n",
    "# =============================================\n",
    "\n",
    "def evaluate_zero_shot_flickr30k(sample_size=None):\n",
    "    \"\"\"\n",
    "    Evaluate BLIP on Zero-Shot Flickr30K Retrieval\n",
    "    Note: This requires access to the Flickr30K dataset\n",
    "\n",
    "    Args:\n",
    "        sample_size: Number of images to evaluate\n",
    "    \"\"\"\n",
    "    from models.blip_itm import blip_itm\n",
    "    import json\n",
    "\n",
    "    print(\"\\nEvaluating Zero-Shot Flickr30K Retrieval...\")\n",
    "\n",
    "    # Check if we have access to Flickr30K dataset\n",
    "    try:\n",
    "        # This is a placeholder for actual Flickr30K dataset loading\n",
    "        # In a real scenario, you'd load the Flickr30K test set\n",
    "        # Since we don't have direct access to it, we'll simulate with placeholder data\n",
    "\n",
    "        # We're creating synthetic data for demonstration\n",
    "        # In a real evaluation, you'd load the actual Flickr30K test set\n",
    "        print(\"Note: Using synthetic Flickr30K data for demonstration\")\n",
    "        flickr_test_size = 1000  # Flickr30K test set size is 1000\n",
    "\n",
    "        # Create synthetic data\n",
    "        if not os.path.exists(f\"{flickr_path}/imgs\"):\n",
    "            os.makedirs(f\"{flickr_path}/imgs\")\n",
    "\n",
    "        # Create synthetic annotation file\n",
    "        flickr_annotations = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": []\n",
    "        }\n",
    "\n",
    "        for i in range(min(sample_size or 100, flickr_test_size)):\n",
    "            flickr_annotations[\"images\"].append({\n",
    "                \"id\": i,\n",
    "                \"file_name\": f\"img_{i}.jpg\"\n",
    "            })\n",
    "\n",
    "            flickr_annotations[\"annotations\"].append({\n",
    "                \"image_id\": i,\n",
    "                \"caption\": f\"This is a synthetic caption for image {i}\"\n",
    "            })\n",
    "\n",
    "        with open(f\"{flickr_path}/annotations/test_annotations.json\", 'w') as f:\n",
    "            json.dump(flickr_annotations, f)\n",
    "\n",
    "        # Since we don't have actual Flickr30K images, we'll reuse COCO images for demonstration\n",
    "        # In a real evaluation, you'd use the actual Flickr30K images\n",
    "        population_size = flickr_test_size\n",
    "\n",
    "        # Calculate appropriate sample size if not provided\n",
    "        if sample_size is None:\n",
    "            sample_size = min(calculate_sample_size(population_size), 200)\n",
    "        print(f\"Using sample size of {sample_size} images (from total of {population_size})\")\n",
    "\n",
    "        # Load BLIP model trained on COCO\n",
    "        image_size = 384\n",
    "        model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n",
    "\n",
    "        model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Paper reported zero-shot results\n",
    "        paper_results = {\n",
    "            'TR@1': 94.8,\n",
    "            'TR@5': 99.7,\n",
    "            'TR@10': 100.0,\n",
    "            'IR@1': 84.9,\n",
    "            'IR@5': 96.7,\n",
    "            'IR@10': 98.3\n",
    "        }\n",
    "\n",
    "        # Since we don't have the actual dataset, we'll just report the paper results\n",
    "        our_results = {\n",
    "            'TR@1': 'N/A (requires Flickr30K dataset)',\n",
    "            'TR@5': 'N/A (requires Flickr30K dataset)',\n",
    "            'TR@10': 'N/A (requires Flickr30K dataset)',\n",
    "            'IR@1': 'N/A (requires Flickr30K dataset)',\n",
    "            'IR@5': 'N/A (requires Flickr30K dataset)',\n",
    "            'IR@10': 'N/A (requires Flickr30K dataset)'\n",
    "        }\n",
    "\n",
    "        # Compare results\n",
    "        comparison = compare_results(paper_results, our_results, \"Zero-Shot Flickr30K Retrieval\",\n",
    "                                    sample_size, population_size)\n",
    "\n",
    "        print(\"\\nNote: Zero-shot Flickr30K evaluation requires the actual Flickr30K dataset.\")\n",
    "        print(\"We're only reporting the paper results for comparison.\")\n",
    "\n",
    "        return comparison, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Flickr30K evaluation: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# =============================================\n",
    "# 5. NLVR2 Evaluation\n",
    "# =============================================\n",
    "\n",
    "def evaluate_nlvr2(sample_size=None):\n",
    "    \"\"\"\n",
    "    Evaluate BLIP on Natural Language Visual Reasoning (NLVR2)\n",
    "\n",
    "    Args:\n",
    "        sample_size: Number of samples to evaluate (if None, automatically calculate)\n",
    "    \"\"\"\n",
    "    from models.med import BertConfig, BertModel\n",
    "    from datasets import load_dataset\n",
    "    import json\n",
    "\n",
    "    print(\"\\nEvaluating Natural Language Visual Reasoning (NLVR2)...\")\n",
    "\n",
    "    try:\n",
    "        # Load NLVR2 dataset from HuggingFace\n",
    "        nlvr2_dataset = load_dataset(\"lmms-lab/NLVR2\")\n",
    "\n",
    "        # Use balanced_dev split for evaluation\n",
    "        dev_dataset = nlvr2_dataset[\"balanced_dev\"]\n",
    "        population_size = len(dev_dataset)\n",
    "\n",
    "        # Calculate appropriate sample size if not provided\n",
    "        if sample_size is None:\n",
    "            sample_size = calculate_sample_size(population_size)\n",
    "        print(f\"Using sample size of {sample_size} samples (from total of {population_size})\")\n",
    "\n",
    "        # Use random subset for evaluation\n",
    "        random.seed(42)  # For reproducibility\n",
    "        indices = random.sample(range(population_size), min(sample_size, population_size))\n",
    "        subset_dataset = [dev_dataset[i] for i in indices]\n",
    "\n",
    "        # Load the BLIP model for NLVR2\n",
    "        image_size = 384\n",
    "        model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_nlvr.pth'\n",
    "\n",
    "        # Define custom BLIP_NLVR2 model (similar to what's in the BLIP repo)\n",
    "        class BLIP_NLVR2(torch.nn.Module):\n",
    "            def __init__(self, med_config='configs/med_config.json', image_size=384, vit='base'):\n",
    "                \"\"\"\n",
    "                Args:\n",
    "                    med_config (str): path for the mixture of encoder-decoder model's configuration file\n",
    "                    image_size (int): input image size\n",
    "                    vit (str): model size of vision transformer\n",
    "                \"\"\"\n",
    "                super().__init__()\n",
    "\n",
    "                # Import necessary models\n",
    "                from models.blip import create_vit, init_tokenizer\n",
    "\n",
    "                self.visual_encoder = create_vit(vit=vit, image_size=image_size)\n",
    "                self.tokenizer = init_tokenizer()\n",
    "\n",
    "                encoder_config = BertConfig.from_json_file(med_config)\n",
    "                encoder_config.encoder_width = self.visual_encoder.width\n",
    "                self.text_encoder = BertModel(config=encoder_config, add_pooling_layer=False)\n",
    "\n",
    "                self.cls_head = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(self.text_encoder.config.hidden_size, self.text_encoder.config.hidden_size),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(self.text_encoder.config.hidden_size, 2)\n",
    "                )\n",
    "\n",
    "                # Load pre-trained weights\n",
    "                checkpoint = torch.load(model_url, map_location='cpu')\n",
    "                state_dict = checkpoint['model']\n",
    "\n",
    "                # Load weights into model (simplified for demonstration)\n",
    "                msg = self.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "            def forward(self, image1, image2, text, targets=None):\n",
    "                # Process image1\n",
    "                image_embeds1 = self.visual_encoder(image1)\n",
    "\n",
    "                # Process image2\n",
    "                image_embeds2 = self.visual_encoder(image2)\n",
    "\n",
    "                # Prepare text input\n",
    "                text_input = self.tokenizer(text, padding='max_length', truncation=True,\n",
    "                                          max_length=35, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                # Process text with image contexts\n",
    "                # This is a simplified implementation - the actual implementation would match the BLIP repo\n",
    "                text_output = self.text_encoder(text_input.input_ids,\n",
    "                                             attention_mask=text_input.attention_mask,\n",
    "                                             encoder_hidden_states=[image_embeds1, image_embeds2],\n",
    "                                             encoder_attention_mask=[\n",
    "                                                 torch.ones(image_embeds1.size()[:-1], dtype=torch.long).to(device),\n",
    "                                                 torch.ones(image_embeds2.size()[:-1], dtype=torch.long).to(device)\n",
    "                                             ],\n",
    "                                             return_dict=True)\n",
    "\n",
    "                # Classification\n",
    "                cls_feats = text_output.last_hidden_state[:, 0]\n",
    "                prediction = self.cls_head(cls_feats)\n",
    "\n",
    "                if targets is not None:\n",
    "                    # Training (not needed for evaluation)\n",
    "                    loss = torch.nn.functional.cross_entropy(prediction, targets)\n",
    "                    return {\"loss\": loss}\n",
    "                else:\n",
    "                    return {\"predictions\": torch.argmax(prediction, dim=1)}\n",
    "\n",
    "        # Note: In practice, loading the NLVR2 model requires the BLIP codebase\n",
    "        # For the purpose of this notebook, we'll just report the paper results\n",
    "        print(\"Note: Actual evaluation requires the complete BLIP codebase with NLVR2 model implementation.\")\n",
    "        print(\"Reporting paper results for comparison.\")\n",
    "\n",
    "        # Define image transform\n",
    "        from torchvision import transforms\n",
    "        from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "\n",
    "        # Reported results from BLIP paper for NLVR2\n",
    "        paper_results = {\n",
    "            'NLVR2 dev': 82.67,\n",
    "            'NLVR2 test-P': 82.30\n",
    "        }\n",
    "\n",
    "        # For demonstration, we'll just report the paper results\n",
    "        our_results = {\n",
    "            'NLVR2 dev': 'N/A (requires full BLIP implementation)',\n",
    "            'NLVR2 test-P': 'N/A (requires full BLIP implementation)'\n",
    "        }\n",
    "\n",
    "        # Compare results\n",
    "        comparison = compare_results(paper_results, our_results, \"NLVR2\",\n",
    "                                   sample_size, population_size)\n",
    "\n",
    "        return comparison, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in NLVR2 evaluation: {str(e)}\")\n",
    "        print(\"To run NLVR2 evaluation, you need to install the 'datasets' package and have the BLIP codebase.\")\n",
    "        return None, None\n",
    "\n",
    "# =============================================\n",
    "# Visualization and Analysis Functions\n",
    "# =============================================\n",
    "\n",
    "def plot_benchmark_comparison(results):\n",
    "    \"\"\"\n",
    "    Plot benchmark results comparison\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary of evaluation results\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Extract paper results and our results\n",
    "    tasks = []\n",
    "    paper_scores = []\n",
    "    our_scores = []\n",
    "\n",
    "    # Image Captioning\n",
    "    if results.get(\"caption_comparison\") is not None:\n",
    "        tasks.append(\"Image Captioning\\n(CIDEr)\")\n",
    "        paper_scores.append(133.3)  # Paper CIDEr score\n",
    "        our_scores.append(results[\"caption_scores\"][\"CIDEr\"] * 100)\n",
    "\n",
    "    # VQA\n",
    "    if results.get(\"vqa_comparison\") is not None:\n",
    "        tasks.append(\"VQA\\n(Accuracy)\")\n",
    "        paper_scores.append(78.25)  # Paper VQA test-dev\n",
    "        our_scores.append(results[\"vqa_accuracy\"] * 100)\n",
    "\n",
    "    # Image-Text Retrieval\n",
    "    if results.get(\"retrieval_comparison\") is not None:\n",
    "        tr_recall, ir_recall = results[\"retrieval_scores\"]\n",
    "\n",
    "        tasks.append(\"Text Retrieval\\n(R@1)\")\n",
    "        paper_scores.append(81.9)  # Paper TR@1\n",
    "        our_scores.append(tr_recall[1])\n",
    "\n",
    "        tasks.append(\"Image Retrieval\\n(R@1)\")\n",
    "        paper_scores.append(64.3)  # Paper IR@1\n",
    "        our_scores.append(ir_recall[1])\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Bar positions\n",
    "    x = np.arange(len(tasks))\n",
    "    width = 0.35\n",
    "\n",
    "    # Plot bars\n",
    "    ax.bar(x - width/2, paper_scores, width, label='Paper Results')\n",
    "    ax.bar(x + width/2, our_scores, width, label='Our Results')\n",
    "\n",
    "    # Add labels, title and legend\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('BLIP Benchmark Comparison', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(tasks)\n",
    "    ax.legend()\n",
    "\n",
    "    # Add values on top of bars\n",
    "    for i, v in enumerate(paper_scores):\n",
    "        ax.text(i - width/2, v + 1, f'{v:.1f}', ha='center')\n",
    "\n",
    "    for i, v in enumerate(our_scores):\n",
    "        ax.text(i + width/2, v + 1, f'{v:.1f}', ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def visualize_image_captioning_examples(results, coco_img_path, coco_ann_path, num_examples=5):\n",
    "    \"\"\"\n",
    "    Visualize image captioning examples\n",
    "\n",
    "    Args:\n",
    "        results: Image captioning results\n",
    "        coco_img_path: Path to COCO images\n",
    "        coco_ann_path: Path to COCO annotations\n",
    "        num_examples: Number of examples to visualize\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pycocotools.coco import COCO\n",
    "\n",
    "    # Load COCO dataset\n",
    "    coco_ann_file = f\"{coco_ann_path}/captions_val2017.json\"\n",
    "    coco = COCO(coco_ann_file)\n",
    "\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(num_examples, 1, figsize=(12, 5 * num_examples))\n",
    "\n",
    "    if num_examples == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Get random examples from results\n",
    "    indices = np.random.choice(len(results), min(num_examples, len(results)), replace=False)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        result = results[idx]\n",
    "        img_id = result['image_id']\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = f\"{coco_img_path}/{img_info['file_name']}\"\n",
    "\n",
    "        # Get ground truth captions\n",
    "        anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n",
    "        gt_captions = [ann['caption'] for ann in anns]\n",
    "\n",
    "        # Display image\n",
    "        img = plt.imread(img_path)\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "        # Display generated caption and ground truth\n",
    "        axs[i].set_title(f\"Generated: {result['caption']}\", fontsize=12)\n",
    "\n",
    "        # Add ground truth captions as text\n",
    "        gt_text = \"Ground Truth:\\n\"\n",
    "        for j, cap in enumerate(gt_captions[:3]):  # Show up to 3 ground truth captions\n",
    "            gt_text += f\"{j+1}. {cap}\\n\"\n",
    "\n",
    "        axs[i].text(0, img.shape[0] + 20, gt_text, fontsize=10, wrap=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# =============================================\n",
    "# Run All Evaluations\n",
    "# =============================================\n",
    "\n",
    "def run_all_evaluations(subset_size=100):\n",
    "    \"\"\"\n",
    "    Run all benchmark evaluations\n",
    "\n",
    "    Args:\n",
    "        subset_size: Number of samples to use for each evaluation\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"BLIP Benchmark Evaluation - Running All Tests\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Note: Using a small subset for demonstration\n",
    "    # For full benchmark verification, use larger subset or entire dataset\n",
    "\n",
    "    # 1. Image Captioning\n",
    "    caption_results, caption_scores = evaluate_blip_hf_captioning(subset_size)\n",
    "\n",
    "    # 2. Visual Question Answering\n",
    "    vqa_comparison, vqa_accuracy, vqa_results = evaluate_vqa(subset_size)\n",
    "\n",
    "    # 3. Image-Text Retrieval\n",
    "    retrieval_comparison, retrieval_scores = evaluate_image_text_retrieval(subset_size)\n",
    "\n",
    "    # 4. Zero-Shot Flickr30K Retrieval\n",
    "    flickr_comparison, _ = evaluate_zero_shot_flickr30k(subset_size)\n",
    "\n",
    "    # 5. NLVR2\n",
    "    try:\n",
    "        nlvr2_comparison, _ = evaluate_nlvr2(subset_size)\n",
    "    except Exception as e:\n",
    "        print(f\"NLVR2 evaluation requires additional setup. Skipping... Error: {str(e)}\")\n",
    "        nlvr2_comparison = None\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"BLIP Benchmark Evaluation - All Tests Completed\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Summarize\n",
    "    print(\"\\nSummary of Benchmark Results:\")\n",
    "    print(f\"Subset size used: {subset_size} samples\")\n",
    "    print(\"1. Image Captioning:\")\n",
    "    print(f\"   BLEU@4: {caption_scores.get('Bleu_4', 'N/A')} (Paper: 39.7%)\")\n",
    "    print(f\"   CIDEr: {caption_scores.get('CIDEr', 'N/A')} (Paper: 133.3)\")\n",
    "\n",
    "    print(\"\\n2. Visual Question Answering:\")\n",
    "    print(f\"   Accuracy: {vqa_accuracy*100:.2f}% (Paper VQA test-dev: 78.25%)\")\n",
    "\n",
    "    print(\"\\n3. Image-Text Retrieval:\")\n",
    "    tr_recall, ir_recall = retrieval_scores\n",
    "    print(f\"   Text Retrieval R@1: {tr_recall[1]:.2f}% (Paper: 81.9%)\")\n",
    "    print(f\"   Image Retrieval R@1: {ir_recall[1]:.2f}% (Paper: 64.3%)\")\n",
    "\n",
    "    print(\"\\n4. Zero-Shot Flickr30K:\")\n",
    "    print(\"   Results require the actual Flickr30K dataset (Paper TR@1: 94.8%, IR@1: 84.9%)\")\n",
    "\n",
    "    print(\"\\n5. NLVR2:\")\n",
    "    print(\"   Results require full BLIP implementation (Paper dev: 82.67%, test-P: 82.30%)\")\n",
    "\n",
    "    print(\"\\nNote: These results are based on a small subset and may not fully match paper results.\")\n",
    "    print(\"For accurate comparison, the full validation sets should be used.\")\n",
    "\n",
    "    return {\n",
    "        \"caption_comparison\": None,  # Placeholder, since we are not explicitly calculating comparison in this function\n",
    "        \"caption_scores\": caption_scores,\n",
    "        \"vqa_comparison\": vqa_comparison,\n",
    "        \"vqa_accuracy\": vqa_accuracy,\n",
    "        \"retrieval_comparison\": retrieval_comparison,\n",
    "        \"retrieval_scores\": retrieval_scores,\n",
    "        \"flickr_comparison\": flickr_comparison,\n",
    "        \"nlvr2_comparison\": nlvr2_comparison\n",
    "    }\n",
    "\n",
    "# Run the evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    # Run all evaluations with the subset size you prefer\n",
    "    results = run_all_evaluations(subset_size=100)  # You can adjust the subset_size as needed\n",
    "\n",
    "    # Optionally, print or process the results\n",
    "    print(\"\\nAll Evaluations Completed!\")\n",
    "\n",
    "    # Assuming results contain captions and references from the function\n",
    "    if results:\n",
    "        print(\"\\nSummary of Image Captioning Results:\")\n",
    "        for result in results:\n",
    "            print(f\"Image ID: {result['image_id']}\")\n",
    "            print(f\"Generated Caption: {result['caption']}\")\n",
    "            print(f\"References: {', '.join(result['caption'] for result in results)}\")  # Displaying references\n",
    "\n",
    "    else:\n",
    "        print(\"Evaluation failed. Please check the errors above.\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivQNyL2GvsyU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
