{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Post-training using Proximal Policy Optimization"
      ],
      "metadata": {
        "id": "XX5Y0eS9yyM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we show how you can apply post training using Proximal Policy Optimization (RL) to a base LLM."
      ],
      "metadata": {
        "id": "dxT63H1hy5nQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, install required packages. TRL is a cutting-edge library designed for post-training foundation models."
      ],
      "metadata": {
        "id": "I8_NejhEzEoZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "XQB6hrMfthTX"
      },
      "outputs": [],
      "source": [
        "!pip install trl==0.4.7 transformers==4.28.1 peft==0.3.0 accelerate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a base model. We only use a very small foundation model as this notebook is for illustration purposes only."
      ],
      "metadata": {
        "id": "mgdhp5BgzW3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "O9NrbdsWtkmI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a toy problem where we ask the model what the capital of France is, and use a rule-based reward model where we give a reward when the answer contains Paris and a punishment when it doesn't."
      ],
      "metadata": {
        "id": "NaFBh1BuzqkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy dataset\n",
        "prompts = [\"What is the capital city of France?\"] * 10\n",
        "\n",
        "# Improved reward function: fuzzy match for 'Paris'\n",
        "def reward_fn(output: str) -> float:\n",
        "    return 1.0 if re.search(r'\\b[Pp]aris\\b', output) else 0.0"
      ],
      "metadata": {
        "id": "J7zf7aXqul-V"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the PPOTrainer"
      ],
      "metadata": {
        "id": "qwUEgRzezp1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_config = PPOConfig(\n",
        "    model_name=model_name,\n",
        "    learning_rate=1e-4,\n",
        "    batch_size=2,\n",
        "    mini_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    target_kl=0.2\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(config=ppo_config, model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "BSMygT8tuqCC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop. We only train for a few epochs as this is only for illustration purposes."
      ],
      "metadata": {
        "id": "ovtYtrqo0BeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import re\n",
        "\n",
        "\n",
        "# Run a few PPO steps (small demo loop)\n",
        "for epoch in range(5):\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    batch_prompts = random.sample(prompts, k=2)\n",
        "\n",
        "    # Tokenize prompts\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
        "    query_tensors = [q for q in inputs[\"input_ids\"]]\n",
        "\n",
        "    # Generate responses with sampling + top-k/top-p\n",
        "    response_tensors = []\n",
        "    for query in query_tensors:\n",
        "        output = model.generate(\n",
        "            query.unsqueeze(0),\n",
        "            max_new_tokens=20,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=1.0,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        response = output[0][query.shape[0]:]  # new tokens only\n",
        "        response_tensors.append(response)\n",
        "\n",
        "    # Decode and compute rewards\n",
        "    responses = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]\n",
        "    rewards = [torch.tensor(reward_fn(r)).to(device) for r in responses]\n",
        "\n",
        "    # PPO update step\n",
        "    ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "\n",
        "    # Print result\n",
        "    for p, r, rw in zip(batch_prompts, responses, rewards):\n",
        "        print(f\"\\nPrompt: {p}\\n→ Response: {repr(r)}\\n→ Reward: {rw.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W9qb7hlx6g5",
        "outputId": "36120d74-0578-4427-e5a3-cc347e6cb3c2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: What is the capital city of France?\n",
            "→ Response: \" The city you're in is not a city. The city you're in is not a city.\"\n",
            "→ Reward: 0.0\n",
            "\n",
            "Prompt: What is the capital city of France?\n",
            "→ Response: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
            "→ Reward: 0.0\n",
            "Epoch 2\n",
            "\n",
            "Prompt: What is the capital city of France?\n",
            "→ Response: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
            "→ Reward: 0.0\n",
            "\n",
            "Prompt: What is the capital city of France?\n",
            "→ Response: '\\n\\n\\n\\n\\n\\n\\n\\nThe UK plans to start the first public funding at 1240'\n",
            "→ Reward: 0.0\n",
            "Epoch 3\n",
            "\n",
            "Prompt: What is the capital city of France?\n",
            "→ Response: '\\n\\n\\n- $1,000; R322175; New York City City City City'\n",
            "→ Reward: 0.0\n",
            "\n",
            "Prompt: What is the capital city of France?\n",
            "→ Response: ' - June 28\\n\\n\\nAaaaaAaAaAAaAa'\n",
            "→ Reward: 0.0\n",
            "Epoch 4\n",
            "\n",
            "Prompt: What is the capital city of France?\n",
            "→ Response: ' A $2002510142243444444444545444445444444'\n",
            "→ Reward: 0.0\n",
            "\n",
            "Prompt: What is the capital city of France?\n",
            "→ Response: '\\n\\nState Public Works. Eighty First City City City City City City City City City City City'\n",
            "→ Reward: 0.0\n",
            "Epoch 5\n",
            "\n",
            "Prompt: What is the capital city of France?\n",
            "→ Response: '\\nWetTACC1G9J6SSKXXXXXX'\n",
            "→ Reward: 0.0\n",
            "\n",
            "Prompt: What is the capital city of France?\n",
            "→ Response: ' Sault Lake Valley Valley Valley Valley Valley Valley Valley Valley Valley Valley Valley Valley Valley Valley Valley Valley Valley'\n",
            "→ Reward: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like the \"distilgpt2\" model is too small to accurately respond to our questions, but there are probably also problems in how we generate from the model."
      ],
      "metadata": {
        "id": "UURewwYR2NWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the underlying language model\n",
        "base_model = model.pretrained_model\n",
        "\n",
        "# Prompt for testing\n",
        "test_prompt = \"What is the capital of France?\"\n",
        "device = next(model.parameters()).device\n",
        "test_input = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate a response\n",
        "output = base_model.generate(\n",
        "    test_input[\"input_ids\"],\n",
        "    max_new_tokens=20,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=1.0,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "print(\"Generated answer after training:\", tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouxYkfWTygb_",
        "outputId": "1ab5bcaf-30bc-46d8-9632-51643ac20dfb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated answer after training: What is the capital of France?\n",
            "4GXM70XPZ6THKKKKKKKK\n"
          ]
        }
      ]
    }
  ]
}