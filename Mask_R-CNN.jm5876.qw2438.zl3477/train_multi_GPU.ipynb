{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mask R-CNN Training Script\n",
        "\n",
        "This notebook demonstrates how to train a Mask R-CNN model using custom training code. It includes:\n",
        "1. Model creation with optional pretrained weights.\n",
        "2. A main function to handle training logic, such as:\n",
        "   - Data loading (COCO or VOC format)\n",
        "   - Distributed training\n",
        "   - Mixed-precision training\n",
        "   - Learning rate scheduling\n",
        "   - Evaluation after each epoch\n",
        "   - Saving checkpoints\n",
        "\n",
        "## Contents\n",
        "1. **Imports**: Necessary libraries and modules.\n",
        "2. **Create Model Function**: Builds Mask R-CNN with a ResNet50-FPN backbone.\n",
        "3. **Main Function**: The core training loop.\n",
        "4. **Argument Parsing**: Command-line arguments for customizing training behavior.\n",
        "\n",
        "### Notes\n",
        "1. Make sure you have the required environment with `torch`, `torchvision`, and other dependencies installed.\n",
        "2. Ensure the supporting scripts/files (`transforms.py`, `my_dataset_coco.py`, `my_dataset_voc.py`, `backbone.py`, `network_files.py`, `train_utils` directory, etc.) are in your Python path or in the same directory.\n",
        "3. If running directly in Jupyter, you might need to manually supply or adjust the argument settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "from torchvision.ops.misc import FrozenBatchNorm2d\n",
        "\n",
        "import transforms\n",
        "from my_dataset_coco import CocoDetection\n",
        "from my_dataset_voc import VOCInstances\n",
        "from backbone import resnet50_fpn_backbone\n",
        "from network_files import MaskRCNN\n",
        "import train_utils.train_eval_utils as utils\n",
        "from train_utils import GroupedBatchSampler, create_aspect_ratio_groups,\n",
        "                       init_distributed_mode, save_on_master, mkdir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Model Function\n",
        "This function builds a Mask R-CNN model with a ResNet50-FPN backbone. By default, it can load COCO-pretrained weights (unless `load_pretrain_weights` is set to `False`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(num_classes, load_pretrain_weights=True):\n",
        "    \"\"\"\n",
        "    Create a Mask R-CNN model with a ResNet50-FPN backbone.\n",
        "    Args:\n",
        "        num_classes (int): Number of classes (excluding background) + 1 = total classes.\n",
        "        load_pretrain_weights (bool): Whether to load COCO-pretrained weights.\n",
        "    Returns:\n",
        "        model (torch.nn.Module): The Mask R-CNN model.\n",
        "    Note:\n",
        "        If GPU memory is limited, consider using FrozenBatchNorm2d instead of nn.BatchNorm2d.\n",
        "    \"\"\"\n",
        "    # Example: freeze layers or set trainable layers as needed.\n",
        "    # Here, we specify trainable_layers=3, meaning the last 3 layers (layer2, layer3, layer4) are trainable.\n",
        "    backbone = resnet50_fpn_backbone(pretrain_path=\"resnet50.pth\", trainable_layers=3)\n",
        "    model = MaskRCNN(backbone, num_classes=num_classes)\n",
        "\n",
        "    if load_pretrain_weights:\n",
        "        # Load the COCO-pretrained weights.\n",
        "        # These weights come from the model trained on COCO dataset.\n",
        "        weights_dict = torch.load(\"./maskrcnn_resnet50_fpn_coco.pth\", map_location=\"cpu\")\n",
        "        # Remove the final prediction layers' parameters (box_predictor, mask_fcn_logits),\n",
        "        # so they won't overwrite our desired layer sizes.\n",
        "        for k in list(weights_dict.keys()):\n",
        "            if (\"box_predictor\" in k) or (\"mask_fcn_logits\" in k):\n",
        "                del weights_dict[k]\n",
        "\n",
        "        # Load partial weights\n",
        "        print(model.load_state_dict(weights_dict, strict=False))\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Training Function\n",
        "This function:\n",
        "1. Initializes distributed training (if applicable).\n",
        "2. Creates data transforms for training and validation.\n",
        "3. Loads COCO or VOC datasets.\n",
        "4. Sets up data loaders with possible aspect ratio grouping.\n",
        "5. Creates the Mask R-CNN model and loads pretrained weights if requested.\n",
        "6. Handles optimizer, learning rate scheduler, and mixed-precision training (AMP).\n",
        "7. Optionally resumes training from a checkpoint.\n",
        "8. Trains for the specified number of epochs, evaluating after each epoch.\n",
        "9. Logs and saves model checkpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    # Initialize distributed training if needed\n",
        "    init_distributed_mode(args)\n",
        "    print(args)\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    # File names for saving COCO evaluation results\n",
        "    now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    det_results_file = f\"det_results{now}.txt\"\n",
        "    seg_results_file = f\"seg_results{now}.txt\"\n",
        "\n",
        "    # Create data transforms\n",
        "    print(\"Loading data\")\n",
        "    data_transform = {\n",
        "        \"train\": transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomHorizontalFlip(0.5)\n",
        "        ]),\n",
        "        \"val\": transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    COCO_root = args.data_path  # Path to COCO dataset root\n",
        "    # Example for VOC:\n",
        "    # data_root = \"/path/to/VOCdevkit\"\n",
        "\n",
        "    # Load training dataset (COCO or VOC)\n",
        "    train_dataset = CocoDetection(COCO_root, \"train\", data_transform[\"train\"])\n",
        "    # Alternatively:\n",
        "    # train_dataset = VOCInstances(data_root, year=\"2012\", txt_name=\"train.txt\")\n",
        "\n",
        "    # Load validation dataset (COCO or VOC)\n",
        "    val_dataset = CocoDetection(COCO_root, \"val\", data_transform[\"val\"])\n",
        "    # Alternatively:\n",
        "    # val_dataset = VOCInstances(data_root, year=\"2012\", txt_name=\"val.txt\")\n",
        "\n",
        "    print(\"Creating data loaders\")\n",
        "    if args.distributed:\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "        test_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
        "    else:\n",
        "        train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
        "        test_sampler = torch.utils.data.SequentialSampler(val_dataset)\n",
        "\n",
        "    # Aspect ratio grouping\n",
        "    if args.aspect_ratio_group_factor >= 0:\n",
        "        group_ids = create_aspect_ratio_groups(train_dataset, k=args.aspect_ratio_group_factor)\n",
        "        train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, args.batch_size)\n",
        "    else:\n",
        "        train_batch_sampler = torch.utils.data.BatchSampler(\n",
        "            train_sampler, args.batch_size, drop_last=True)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_sampler=train_batch_sampler,\n",
        "        num_workers=args.workers,\n",
        "        collate_fn=train_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=1,\n",
        "        sampler=test_sampler,\n",
        "        num_workers=args.workers,\n",
        "        collate_fn=train_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    print(\"Creating model\")\n",
        "    # Create model; num_classes includes background, so we add 1\n",
        "    model = create_model(num_classes=args.num_classes + 1, load_pretrain_weights=args.pretrain)\n",
        "    model.to(device)\n",
        "\n",
        "    # Convert BatchNorm to SyncBatchNorm if requested (for multi-GPU)\n",
        "    if args.distributed and args.sync_bn:\n",
        "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
        "\n",
        "    model_without_ddp = model\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model,\n",
        "            device_ids=[args.gpu]\n",
        "        )\n",
        "        model_without_ddp = model.module\n",
        "\n",
        "    # Collect trainable parameters\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params,\n",
        "        lr=args.lr,\n",
        "        momentum=args.momentum,\n",
        "        weight_decay=args.weight_decay\n",
        "    )\n",
        "\n",
        "    # AMP scaler for mixed precision\n",
        "    scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
        "\n",
        "    # Example: MultiStepLR for learning rate scheduling\n",
        "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer,\n",
        "        milestones=args.lr_steps,\n",
        "        gamma=args.lr_gamma\n",
        "    )\n",
        "\n",
        "    # Resume from a checkpoint if provided\n",
        "    if args.resume:\n",
        "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "        args.start_epoch = checkpoint['epoch'] + 1\n",
        "        if args.amp and \"scaler\" in checkpoint:\n",
        "            scaler.load_state_dict(checkpoint[\"scaler\"])\n",
        "\n",
        "    # If test_only is specified, run evaluation and exit\n",
        "    if args.test_only:\n",
        "        utils.evaluate(model, data_loader_test, device=device)\n",
        "        return\n",
        "\n",
        "    # Lists for tracking metrics\n",
        "    train_loss = []\n",
        "    learning_rate = []\n",
        "    val_map = []\n",
        "\n",
        "    print(\"Start training\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        # For distributed training, set epoch for the sampler\n",
        "        if args.distributed:\n",
        "            train_sampler.set_epoch(epoch)\n",
        "\n",
        "        # Train for one epoch\n",
        "        mean_loss, lr = utils.train_one_epoch(\n",
        "            model,\n",
        "            optimizer,\n",
        "            data_loader,\n",
        "            device,\n",
        "            epoch,\n",
        "            args.print_freq,\n",
        "            warmup=True,\n",
        "            scaler=scaler\n",
        "        )\n",
        "\n",
        "        # Step the scheduler to update the learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Evaluate model after each epoch\n",
        "        det_info, seg_info = utils.evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "        # Only record and save on the main process\n",
        "        if args.rank in [-1, 0]:\n",
        "            train_loss.append(mean_loss.item())\n",
        "            learning_rate.append(lr)\n",
        "            # det_info[1] often corresponds to mAP (depending on how evaluate is implemented)\n",
        "            val_map.append(det_info[1])\n",
        "\n",
        "            # Write detection results to file\n",
        "            with open(det_results_file, \"a\") as f:\n",
        "                result_info = [f\"{i:.4f}\" for i in det_info + [mean_loss.item()]] + [f\"{lr:.6f}\"]\n",
        "                txt = \"epoch:{} {}\".format(epoch, '  '.join(result_info))\n",
        "                f.write(txt + \"\\n\")\n",
        "\n",
        "            # Write segmentation results to file\n",
        "            with open(seg_results_file, \"a\") as f:\n",
        "                result_info = [f\"{i:.4f}\" for i in seg_info + [mean_loss.item()]] + [f\"{lr:.6f}\"]\n",
        "                txt = \"epoch:{} {}\".format(epoch, '  '.join(result_info))\n",
        "                f.write(txt + \"\\n\")\n",
        "\n",
        "        # Save checkpoints if an output directory is specified\n",
        "        if args.output_dir:\n",
        "            save_files = {\n",
        "                'model': model_without_ddp.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                'args': args,\n",
        "                'epoch': epoch\n",
        "            }\n",
        "            if args.amp:\n",
        "                save_files[\"scaler\"] = scaler.state_dict()\n",
        "\n",
        "            save_on_master(\n",
        "                save_files,\n",
        "                os.path.join(args.output_dir, f'model_{epoch}.pth')\n",
        "            )\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))\n",
        "\n",
        "    # Plot curves only on the main process\n",
        "    if args.rank in [-1, 0]:\n",
        "        if len(train_loss) != 0 and len(learning_rate) != 0:\n",
        "            from plot_curve import plot_loss_and_lr\n",
        "            plot_loss_and_lr(train_loss, learning_rate)\n",
        "\n",
        "        if len(val_map) != 0:\n",
        "            from plot_curve import plot_map\n",
        "            plot_map(val_map)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Argument Parsing and Entry Point\n",
        "Here we define command-line arguments for training. In typical use, you'd run this script from a terminal, e.g.:\n",
        "```\n",
        "python train_script.py --data-path /data/coco2017 --epochs 30 --batch-size 8\n",
        "```\n",
        "In a notebook environment, you can simulate arguments by providing `args` manually or adjusting them as needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Mask R-CNN Training\")\n",
        "\n",
        "    # Path to the root of your dataset, e.g. COCO\n",
        "    parser.add_argument('--data-path', default='/data/coco2017', help='Path to dataset root')\n",
        "    # Device (e.g. 'cuda' or 'cpu')\n",
        "    parser.add_argument('--device', default='cuda', help='Device (cuda or cpu)')\n",
        "    # Number of classes (excluding background). Example: 80 for COCO.\n",
        "    parser.add_argument('--num-classes', default=90, type=int, help='Number of object classes (excl. background)')\n",
        "    # Batch size per GPU\n",
        "    parser.add_argument('-b', '--batch-size', default=4, type=int,\n",
        "                        help='Images per GPU; total batch size is num_GPU x batch_size')\n",
        "    # Starting epoch if resuming training\n",
        "    parser.add_argument('--start_epoch', default=0, type=int, help='Start epoch')\n",
        "    # Total training epochs\n",
        "    parser.add_argument('--epochs', default=26, type=int, metavar='N',\n",
        "                        help='Number of total epochs to run')\n",
        "    # Number of data loading workers\n",
        "    parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
        "                        help='Number of data loading workers (default: 4)')\n",
        "    # Initial learning rate\n",
        "    parser.add_argument('--lr', default=0.005, type=float,\n",
        "                        help='Initial learning rate (adjust according to GPU count and batch size)')\n",
        "    # Momentum for SGD\n",
        "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
        "                        help='Momentum')\n",
        "    # Weight decay for SGD\n",
        "    parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n",
        "                        metavar='W', help='Weight decay (default: 1e-4)',\n",
        "                        dest='weight_decay')\n",
        "    # LR scheduler step size (if using StepLR)\n",
        "    parser.add_argument('--lr-step-size', default=8, type=int,\n",
        "                        help='Decrease LR every step-size epochs (StepLR)')\n",
        "    # Milestones for MultiStepLR\n",
        "    parser.add_argument('--lr-steps', default=[16, 22], nargs='+', type=int,\n",
        "                        help='Decrease LR at these milestone epochs (MultiStepLR)')\n",
        "    # LR gamma for the scheduler\n",
        "    parser.add_argument('--lr-gamma', default=0.1, type=float,\n",
        "                        help='LR multiply factor at each milestone (default=0.1)')\n",
        "    # Print frequency\n",
        "    parser.add_argument('--print-freq', default=50, type=int, help='Print frequency')\n",
        "    # Output directory for saving checkpoints\n",
        "    parser.add_argument('--output-dir', default='./multi_train', help='Directory to save outputs')\n",
        "    # Resume from a checkpoint\n",
        "    parser.add_argument('--resume', default='', help='Path to the checkpoint to resume from')\n",
        "    # Aspect ratio group factor\n",
        "    parser.add_argument('--aspect-ratio-group-factor', default=3, type=int,\n",
        "                        help='Aspect ratio group factor; set <0 to disable')\n",
        "    # If true, only evaluate and exit\n",
        "    parser.add_argument('--test-only', action='store_true', help='Only run evaluation')\n",
        "\n",
        "    # Distributed training parameters\n",
        "    parser.add_argument('--world-size', default=4, type=int,\n",
        "                        help='Number of distributed processes')\n",
        "    parser.add_argument('--dist-url', default='env://',\n",
        "                        help='URL used to set up distributed training')\n",
        "    parser.add_argument('--sync-bn', dest='sync_bn', type=bool, default=False,\n",
        "                        help='Use synchronized batch norm')\n",
        "    parser.add_argument('--pretrain', type=bool, default=True,\n",
        "                        help='Whether to load COCO pretrained weights')\n",
        "    parser.add_argument('--amp', default=False, action='store_true',\n",
        "                        help='Use torch.cuda.amp for mixed precision training')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Create output directory if it does not exist\n",
        "    if args.output_dir:\n",
        "        mkdir(args.output_dir)\n",
        "\n",
        "    # Run the main function\n",
        "    main(args)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}